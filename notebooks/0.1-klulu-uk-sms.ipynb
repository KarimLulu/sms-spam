{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.config\n",
    "%aimport src.helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from xml.etree.ElementTree import iterparse\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "import numpy as np\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import data_dir\n",
    "from src.helpers import calc_metrics, plot_tfidf_classfeats_h, top_feats_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process raw SMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"karim-sms-allow.xml\"\n",
    "source = data_dir / filename\n",
    "data = []\n",
    "for event, elem in iterparse(source):\n",
    "    if elem.tag == \"sms\":\n",
    "        #if any(elem.attrib[\"body\"]==r[\"text\"] for r in data):\n",
    "        #    continue\n",
    "        record = {}\n",
    "        record[\"text\"] = elem.attrib[\"body\"]\n",
    "        record[\"contact_name\"] = elem.attrib[\"contact_name\"]\n",
    "        record[\"address\"] = elem.attrib[\"address\"]\n",
    "        record[\"timestamp\"] = int(elem.attrib[\"date\"])\n",
    "        record[\"type\"] = elem.attrib[\"type\"]\n",
    "        data.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_excel(data_dir / \"karim-sms-allow.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_filename = \"karim-sms-allow-labeled.xlsx\"\n",
    "labeled = pd.read_excel(data_dir / labeled_filename, sheet_name=\"total sms\")\n",
    "labeled[\"timestamp\"] = (labeled[\"timestamp\"] / 1000).map(datetime.fromtimestamp)\n",
    "labeled[\"resp\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapp = {\"ham\": 0, \"spam\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_filename = \"SMS Data Collection (Responses).xlsx\"\n",
    "responses = pd.read_excel(data_dir / responses_filename)\n",
    "responses = responses.rename(columns={\"SMS text\": \"text\", \n",
    "                                      \"Is it a spam or ham?\": \"label\",\n",
    "                                     \"Timestamp\": \"timestamp\"})\n",
    "responses[\"resp\"] = 1\n",
    "responses[\"label\"] = responses[\"label\"].map(lambda x: mapp.get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat([labeled, responses], ignore_index=True)\n",
    "total.to_excel(data_dir / \"sms-uk-total.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3497, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    78.81\n",
       "1    21.19\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3494, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensionality and class imbalance\n",
    "total.shape\n",
    "total.label.value_counts(normalize=True).round(5)*100\n",
    "total.text.isnull().sum()\n",
    "total = total.loc[total.text.notnull()]\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1938,
   "metadata": {},
   "outputs": [],
   "source": [
    "total[\"text_rep\"] = total[\"text\"].str.replace(r\"[\\(\\d][\\d\\s\\(\\)-]{8,15}\\d\", \"PHONE_NUMBER\", flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1939,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of train: 2445, Num. of test: 1049\n"
     ]
    }
   ],
   "source": [
    "X = total[\"text_rep\"]\n",
    "y = total[\"label\"]\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "                                                    stratify=y)\n",
    "print(f\"Num. of train: {len(X_train)}, Num. of test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1950,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(X_train, X_test, var=\"text\", features=None, vectorizer=None):\n",
    "    f_train = []\n",
    "    f_test = []\n",
    "    for feature in features:\n",
    "        if feature == \"tfidf\":\n",
    "            tf_train = vectorizer.fit_transform(X_train).toarray()\n",
    "            tf_test = vectorizer.transform(X_test).toarray()\n",
    "            f_train.append(tf_train)\n",
    "            f_test.append(tf_test)\n",
    "        if feature == \"length\":\n",
    "            if \"tfidf\" in features:\n",
    "                train = (tf_train>0).sum(axis=1)[:, np.newaxis]\n",
    "                test = (tf_test>0).sum(axis=1)[:, np.newaxis]\n",
    "            else:\n",
    "                train = X_train.map(len).values[:, np.newaxis]\n",
    "                test = X_test.map(len).values[:, np.newaxis]\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        if feature == \"patt\":\n",
    "            patt = \"%|taxi|скидк|цін\"\n",
    "            train = (X_train.str.contains(patt, regex=True, flags=re.I)\n",
    "                     .astype(int).values[:, np.newaxis])\n",
    "            test = (X_test.str.contains(patt, regex=True, flags=re.I)\n",
    "                    .astype(int).values[:, np.newaxis])\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        if feature == \"phone\":\n",
    "            patt = \"PHONE_NUMBER\"\n",
    "            train = X_train.map(lambda x: len(re.findall(patt, x))>0).values[:, np.newaxis]\n",
    "            test = X_test.map(lambda x: len(re.findall(patt, x))>0).values[:, np.newaxis]\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "    return np.concatenate((f_train), axis=1), np.concatenate((f_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1951,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_params = {\"lowercase\": True,\n",
    "             \"analyzer\": \"char_wb\",\n",
    "             \"stop_words\": None,\n",
    "             \"ngram_range\": (4, 4),\n",
    "             \"min_df\": 0.0,\n",
    "             \"max_df\": 1.0,\n",
    "             \"preprocessor\": None,#Preprocessor(),\n",
    "             \"max_features\": 3500,\n",
    "             \"norm\": \"l2\"*0,\n",
    "             \"use_idf\": 0\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 100\n",
    "r = tfidf_train.toarray().sum(axis=1)\n",
    "topn_ids = np.argsort(r)[::-1][:top]\n",
    "voc = [f for i,f in enumerate(features) if i not in topn_ids]\n",
    "tf_params[\"vocabulary\"] = None#voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1952,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(**tf_params)\n",
    "tfidf_train = vectorizer.fit_transform(X_train)\n",
    "tfidf_test = vectorizer.transform(X_test)\n",
    "features = [\n",
    "            \"tfidf\", \n",
    "            \"length\",\n",
    "            \"phone\",\n",
    "            \"patt\",\n",
    "]\n",
    "train, test = build_features(X_train, X_test, features=features, vectorizer=vectorizer, var=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "dfs = top_feats_by_class(tfidf_train, y_train, features, min_tfidf=0.1, top_n=25)\n",
    "plot_tfidf_classfeats_h(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it is much worse to misclassify ham\n",
    "SMS than letting spam pass the filter. So, it is desirable to be able to bias\n",
    "the filter towards classifying SMS as ham, yielding higher precision at the expense of recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(tf, X_test, clf, w=1.5):\n",
    "    probas = clf.predict_proba(X_test)\n",
    "    ratios = np.log(probas[:, 1] ) - np.log(probas[:, 0])\n",
    "    lengths = (tf.toarray()>0).sum(axis=1).T\n",
    "    thresholds = lengths * np.log(w)\n",
    "    y_pred = np.zeros_like(y_test)\n",
    "    y_pred[ratios>thresholds] = 1\n",
    "    return y_pred, ratios, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(min_samples_leaf=5, min_samples_split=15,\n",
    "                             n_estimators=100, max_depth=20, max_features=\"auto\", \n",
    "                             class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1966,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=25,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1966,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.994\n",
      "Recall: 0.937\n",
      "Precision: 0.986\n",
      "F1: 0.961\n",
      "Accuracy: 0.984\n",
      "\n",
      "Confusion matrix:\n",
      "      pred_ham  pred_spam\n",
      "ham        824          3\n",
      "spam        14        208\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       827\n",
      "          1       0.99      0.94      0.96       222\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=25, class_weight=\"balanced\", \n",
    "                         C=1, penalty=\"l2\")\n",
    "#clf = MultinomialNB(alpha=4)#, class_prior=[0.5, 0.5])\n",
    "clf.fit(train, y_train)\n",
    "pred, ratios, thresholds = predict_class(tfidf_test, test, clf, w=1.2)\n",
    "pred = clf.predict(test)\n",
    "proba = clf.predict_proba(test)[:, 1]\n",
    "output, report, conf_matrix = calc_metrics(y_test, pred, proba, labels=[\"ham\", \"spam\"], \n",
    "                                           print_=True, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 32, 281, 310, 327, 394, 424, 456, 457, 668, 804, 943])"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_i = np.where((pred==1) & (y_test==0))[0]\n",
    "fn_i = np.where((pred==0) & (y_test==1))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "#params['scale_pos_weight'] = sum(y_train==0) / sum(y_train==1)\n",
    "params['learning_rate'] = 0.1\n",
    "params['n_estimators'] = 1000\n",
    "params['max_depth'] = 5\n",
    "params['min_child_weight'] = 100\n",
    "params['gamma'] = 0\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.8\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['seed'] = 27\n",
    "params['n_jobs'] = -1\n",
    "params[\"eval_metric\"] = [\"error\",\"auc\"]\n",
    "params[\"early_stopping_rounds\"] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train, y_train)\n",
    "dtest = xgb.DMatrix(test, y_test)\n",
    "eval_set = [(dtrain, \"train\"), (dtest, \"eval\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.212165\ttrain-auc:0.817627\teval-error:0.211731\teval-auc:0.80761\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "[50]\ttrain-error:0.188551\ttrain-auc:0.843192\teval-error:0.197425\teval-auc:0.825158\n",
      "Stopping. Best iteration:\n",
      "[20]\ttrain-error:0.212165\ttrain-auc:0.842316\teval-error:0.211731\teval-auc:0.828377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(dtrain=dtrain, num_boost_round=params.get(\"n_estimators\"), \n",
    "                  early_stopping_rounds=params.get(\"early_stopping_rounds\"), \n",
    "                  params=params, evals=eval_set, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.825\n",
      "Recall: 0.345\n",
      "Precision: 0.554\n",
      "F1: 0.425\n",
      "Accuracy: 0.803\n",
      "\n",
      "Confusion matrix:\n",
      "      pred_ham  pred_spam\n",
      "ham        510         41\n",
      "spam        97         51\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.93      0.88       551\n",
      "          1       0.55      0.34      0.43       148\n",
      "\n",
      "avg / total       0.78      0.80      0.78       699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_proba_xgb = model.predict(dtest)\n",
    "pred_xgb = np.zeros_like(pred_proba_xgb)\n",
    "pred_xgb[pred_proba_xgb>=0.5] = 1\n",
    "xgb_metrics = calc_metrics(y_test, pred_xgb, pred_proba_xgb, mode=\"binary\",\n",
    "                          labels=[\"ham\", \"spam\"], print_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsquash(X):\n",
    "    ''' (n,) -> (n,1) '''\n",
    "    if len(X.shape) == 1 or X.shape[0] == 1:\n",
    "        return np.asarray(X).reshape((len(X), 1))\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(X):\n",
    "    ''' (n,1) -> (n,) '''\n",
    "    return np.squeeze(np.asarray(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(TransformerMixin):\n",
    "    '''Base class for pure transformers'''\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTransformer(TransformerMixin):\n",
    "    ''' Use model predictions as transformer '''\n",
    "    def __init__(self, model, probs=True):\n",
    "        self.model = model\n",
    "        self.probs = probs\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict(model=self.model, probs=self.probs)\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        if self.probs:\n",
    "            pred = self.model.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            pred = self.model.predict(X)\n",
    "        return unsquash(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter(Transformer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            return X\n",
    "        elif isinstance(X, pd.Series):\n",
    "            return X.values\n",
    "        elif isinstance(X, str):\n",
    "            return np.array([X])\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1687,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdfLen(Transformer):\n",
    "    def __init__(self, add_len=True, **tfidf_params):\n",
    "        self.add_len = add_len\n",
    "        self.tfidf_params = tfidf_params\n",
    "        self.vectorizer = TfidfVectorizer(**self.tfidf_params)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        output = self.tfidf_params\n",
    "        output.update({\"add_len\": self.add_len})\n",
    "        return output\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        res = self.vectorizer.transform(X).toarray()\n",
    "        if self.add_len:\n",
    "            lens = unsquash(np.count_nonzero(res, axis=1))\n",
    "            res = np.concatenate((res, lens), axis=1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1692,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_test.iloc[:1]#.values\n",
    "l = TfIdfLen(add_len=1, **tf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1751,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchPattern(Transformer):\n",
    "    \n",
    "    def __init__(self, pattern, is_len, flags=re.U):\n",
    "        self.pattern = pattern\n",
    "        self.is_len = is_len\n",
    "        self.flags = flags\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(pattern=self.pattern, is_len=self.is_len, flags=self.flags)\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        if self.is_len:\n",
    "            func = lambda text: len(re.findall(self.pattern, text, self.flags))\n",
    "        else:\n",
    "            func = lambda text: bool(re.search(self.pattern, text, self.flags))\n",
    "        rez = np.vectorize(func)(X).astype(int)\n",
    "        return unsquash(rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1768,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder(object):\n",
    "\n",
    "    def __init__(self, type_=\"expanding\", na_as_category=True, na_value=-999.0,\n",
    "                 columns=None, feature_names=None):\n",
    "        self.na_as_category = na_as_category\n",
    "        self.na_value = na_value\n",
    "        self.columns = columns\n",
    "        self.feature_names = feature_names\n",
    "        self.mapping = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        self._train = True\n",
    "        self._fitted = False\n",
    "        _, mapping = self.target_encode(X, y, mapping=self.mapping, cols=self.columns,\n",
    "                                        na_as_category=self.na_as_category, na_value=self.na_value)\n",
    "        self.mapping = mapping\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if not self._fitted:\n",
    "            raise ValueError('Must train encoder before it can be used to transform data.')\n",
    "        assert (y is None or X.shape[0] == y.shape[0])\n",
    "        X, mapping = self.target_encode(X, y, mapping=self.mapping, cols=self.columns,\n",
    "                                        na_as_category=self.na_as_category,\n",
    "                                        na_value=self.na_value)\n",
    "        return X\n",
    "\n",
    "    def target_encode(self, X_in, y, mapping=None, cols=None, na_as_category=None, na_value=\"-999\"):\n",
    "        X = X_in.copy(deep=True)\n",
    "        if cols is None:\n",
    "            cols = X.columns.values\n",
    "#         if na_as_category:\n",
    "#             #self.converter.columns = cols\n",
    "#             self.nafiller.columns = cols\n",
    "#             X = self.nafiller(X)\n",
    "        if mapping is not None and self._fitted:\n",
    "            mapping_out = mapping\n",
    "            index = X.index\n",
    "            for el in self.mapping:\n",
    "                col = el[\"col\"]\n",
    "                length = get_len(col)\n",
    "                default = \"_\".join(col)+\"_tmp\"\n",
    "                new_col = self.feature_names.get(col, default)\n",
    "                if self._train:\n",
    "                    grouper = el.pop(\"grouper\")\n",
    "                    cumsum = grouper.cumsum() - y\n",
    "                    cumcnt = grouper.cumcount()\n",
    "                    X[new_col] = cumsum * 1.0 / cumcnt\n",
    "                    X[new_col] = X[new_col].fillna(self._mean)\n",
    "                else:\n",
    "                    X[new_col] = [el[\"mean\"].get(get_slice(record, length), self._mean) for record in\n",
    "                                  X[list(col)].itertuples(index=False, name=None)]\n",
    "            if self._train:\n",
    "                self._train = False\n",
    "        else:\n",
    "            self._mean = y.mean()\n",
    "            mapping_out = []\n",
    "            for col in cols:\n",
    "                grouper = pd.concat([X, y], axis=1).groupby(col)[get_target_name(y)]\n",
    "                means = grouper.agg(\"mean\").to_dict()\n",
    "                mapping_out.append({\"col\": col, \"grouper\": grouper, \"mean\": means})\n",
    "        return X, mapping_out\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleBinaryClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "\n",
    "    def __init__(self, mode, weights=None):\n",
    "        self.mode = mode\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        ''' Predict (weighted) probabilities '''\n",
    "        probs = np.average(X, axis=1, weights=self.weights)\n",
    "        return np.column_stack((1-probs, probs))\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Predict class labels. '''\n",
    "        if self.mode == 'average':\n",
    "            return binarize(self.predict_proba(X)[:,[1]], 0.5)\n",
    "        else:\n",
    "            res = binarize(X, 0.5)\n",
    "            return np.apply_along_axis(lambda x: np.bincount(x.astype(int), self.weights).argmax(), axis=1, arr=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ensemble(model_list, estimator=None):\n",
    "    models = []\n",
    "    for i, model in enumerate(model_list):\n",
    "        models.append(('model_transform'+str(i), ModelTransformer(model)))\n",
    "\n",
    "    if not estimator:\n",
    "        return FeatureUnion(models)\n",
    "    else:\n",
    "        return Pipeline([\n",
    "            ('features', FeatureUnion(models)),\n",
    "            ('estimator', estimator)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squash(Transformer):\n",
    "    def transform(self, X, **kwargs):\n",
    "        return squash(X)\n",
    "\n",
    "class Unsquash(Transformer):\n",
    "    def transform(self, X, **kwargs):\n",
    "        return unsquash(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_pipe(add_len=True, **tfidf_params):\n",
    "    vectorizer = TfIdfLen(add_len, **tfidf_params)\n",
    "    vec_pipe = [\n",
    "        ('vectorizer', vectorizer)]\n",
    "    return Pipeline(vec_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_pipe(patterns):\n",
    "    pipes = []\n",
    "    for i, (patt, params) in enumerate(patterns):\n",
    "        kwargs = params.copy()\n",
    "        name = kwargs.pop(\"name\") + \"_\" + str(i)\n",
    "        temp = MatchPattern(pattern=patt, **kwargs)\n",
    "        pipes.append((name, temp))\n",
    "    return pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1971,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\\\(\\\\d][\\\\d\\\\s\\\\(\\\\)-]{8,15}\\\\d'"
      ]
     },
     "execution_count": 1971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"[\\(\\d][\\d\\s\\(\\)-]{8,15}\\d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1972,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERNS = [(r\"PHONE_NUMBER\", {\"name\": \"phone\",\n",
    "                                            \"is_len\": 0}),\n",
    "           (r\"%|taxi|скидк|цін\", {\"name\": \"custom\",\n",
    "                                  \"is_len\": 0,\n",
    "                                  \"flags\": re.I | re.U})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1981,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_pipe = get_vec_pipe(**tf_params)\n",
    "\n",
    "patt_pipe = get_pattern_pipe(PATTERNS)\n",
    "\n",
    "clf_log = LogisticRegression(random_state=25, class_weight=\"balanced\", \n",
    "                             C=0.1, penalty=\"l2\")\n",
    "clf_nb = MultinomialNB(alpha=0.1)#, class_prior=[0.5, 0.5])\n",
    "clf_e = EnsembleBinaryClassifier(mode=\"averag\", weights=[5, 5])  \n",
    "\n",
    "chain = [\n",
    "    ('converter', Converter()),\n",
    "    ('union', FeatureUnion([\n",
    "    ('vec', vec_pipe),\n",
    "    *patt_pipe\n",
    "    ])),\n",
    "    #(\"ff\", build_ensemble([clf_log, clf_nb], clf_e))\n",
    "]\n",
    "chain.append(('estimator', clf_log))\n",
    "pipe = Pipeline(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1997,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability ham: 80.978%\n",
      "Probability spam: 19.022%\n"
     ]
    }
   ],
   "source": [
    "sms = \"привіт заходь до нас у ввечері додому\"\n",
    "ham, spam = pipe.predict_proba(sms)[0]\n",
    "print(f\"Probability ham: {ham*100:0.3f}%\\nProbability spam: {spam*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1987,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('converter', <__main__.Converter object at 0x7fecd12a3438>), ('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('vec', Pipeline(memory=None,\n",
       "     steps=[('vectorizer', <__main__.TfIdfLen object at 0x7fecd12a3208>)])), ('phone_0', <__main__.MatchPattern object at 0x7fecd12a3358>), ('c...alty='l2', random_state=25,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 1987,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.994\n",
      "Recall: 0.941\n",
      "Precision: 0.968\n",
      "F1: 0.954\n",
      "Accuracy: 0.981\n",
      "\n",
      "Confusion matrix:\n",
      "      pred_ham  pred_spam\n",
      "ham        820          7\n",
      "spam        13        209\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99       827\n",
      "          1       0.97      0.94      0.95       222\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "output, report, conf_matrix = calc_metrics(y_test, pred, proba, labels=[\"ham\", \"spam\"], \n",
    "                                           print_=True, mode=\"binary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
