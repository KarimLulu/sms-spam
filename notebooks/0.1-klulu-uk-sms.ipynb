{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.config\n",
    "%aimport src.helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from xml.etree.ElementTree import iterparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from functools import partial\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import data_dir\n",
    "from src.helpers import calc_metrics, plot_tfidf_classfeats_h, top_feats_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process raw SMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"karim-sms-allow.xml\"\n",
    "source = data_dir / filename\n",
    "data = []\n",
    "for event, elem in iterparse(source):\n",
    "    if elem.tag == \"sms\":\n",
    "        #if any(elem.attrib[\"body\"]==r[\"text\"] for r in data):\n",
    "        #    continue\n",
    "        record = {}\n",
    "        record[\"text\"] = elem.attrib[\"body\"]\n",
    "        record[\"contact_name\"] = elem.attrib[\"contact_name\"]\n",
    "        record[\"address\"] = elem.attrib[\"address\"]\n",
    "        record[\"timestamp\"] = int(elem.attrib[\"date\"])\n",
    "        record[\"type\"] = elem.attrib[\"type\"]\n",
    "        data.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_excel(data_dir / \"karim-sms-allow.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(filenames, file_out, date_format=\"%m-%d-%Y %H:%M:%S\", is_save=1):\n",
    "    output = []\n",
    "    for k,v in filenames.items():\n",
    "        if k == \"labeled\":\n",
    "            df = pd.read_excel(data_dir / v, sheet_name=\"total sms\")\n",
    "            df[\"timestamp\"] = (df[\"timestamp\"] / 1000).map(datetime.fromtimestamp)\n",
    "            df[\"resp\"] = 0\n",
    "            output.append(df)\n",
    "        elif k == \"labeled_1\":\n",
    "            df = pd.read_excel(data_dir / v)\n",
    "            df[\"resp\"] = 0\n",
    "            df[\"timestamp\"] = df[\"timestamp\"].map(lambda x: datetime.strptime(x, date_format))\n",
    "            exclude = [\"Karimushka\"]\n",
    "            df = df.loc[~(df.contact_name.isin(exclude))]\n",
    "            output.append(df)\n",
    "        else:\n",
    "            df = pd.read_excel(data_dir / v)\n",
    "            df = df.rename(columns={\"SMS text\": \"text\", \n",
    "                                    \"Is it a spam or ham?\": \"label\",\n",
    "                                    \"Timestamp\": \"timestamp\"})\n",
    "            df[\"resp\"] = 1\n",
    "            df[\"label\"] = df[\"label\"].map(lambda x: LABEL_MAP.get(x, x))\n",
    "            output.append(df)\n",
    "    df = pd.concat(output, ignore_index=True)\n",
    "    if is_save:\n",
    "        total.to_excel(data_dir / file_out)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\"ham\": 0, \"spam\": 1}\n",
    "FILES = {\"labeled\": \"karim-sms-allow-labeled.xlsx\",\n",
    "         \"labeled_1\": \"tanya-sms-all.xlsx\",\n",
    "         \"responses\": \"SMS Data Collection (Responses).xlsx\"}\n",
    "file_out = \"sms-uk-total.xlsx\"\n",
    "total = build_dataset(FILES, file_out=file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6107, 8)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    80.138\n",
       "1    19.862\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6104, 8)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensionality and class imbalance\n",
    "total.shape\n",
    "total.label.value_counts(normalize=True).round(5)*100\n",
    "total.text.isnull().sum()\n",
    "total = total.loc[total.text.notnull()]\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_excel(data_dir / file_out)\n",
    "total = total.loc[total.text.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total[\"text_rep\"] = total[\"text\"].str.replace(r\"[\\(\\d][\\d\\s\\(\\)-]{8,15}\\d\", \"PHONE_NUMBER\", flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "total[\"text\"] = total[\"text\"].str.replace(r\"[\\n\\r]+\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of train: 4272, Num. of test: 1831\n"
     ]
    }
   ],
   "source": [
    "X = total[\"text\"]\n",
    "y = total[\"label\"]\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "                                                    stratify=y)\n",
    "print(f\"Num. of train: {len(X_train)}, Num. of test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(X_train, X_test, var=\"text\", features=None, vectorizer=None):\n",
    "    f_train = []\n",
    "    f_test = []\n",
    "    for feature in features:\n",
    "        if feature == \"tfidf\":\n",
    "            tf_train = vectorizer.fit_transform(X_train).toarray()\n",
    "            tf_test = vectorizer.transform(X_test).toarray()\n",
    "            f_train.append(tf_train)\n",
    "            f_test.append(tf_test)\n",
    "        if feature == \"length\":\n",
    "            if \"tfidf\" in features:\n",
    "                train = (tf_train>0).sum(axis=1)[:, np.newaxis]\n",
    "                test = (tf_test>0).sum(axis=1)[:, np.newaxis]\n",
    "            else:\n",
    "                train = X_train.map(len).values[:, np.newaxis]\n",
    "                test = X_test.map(len).values[:, np.newaxis]\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        if feature == \"patt\":\n",
    "            patt = \"%|taxi|скидк|цін\"\n",
    "            train = (X_train.str.contains(patt, regex=True, flags=re.I)\n",
    "                     .astype(int).values[:, np.newaxis])\n",
    "            test = (X_test.str.contains(patt, regex=True, flags=re.I)\n",
    "                    .astype(int).values[:, np.newaxis])\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "        if feature == \"phone\":\n",
    "            patt = r\"[\\(\\d][\\d\\s\\(\\)-]{8,15}\\d\"\n",
    "            train = X_train.map(lambda x: len(re.findall(patt, x))>0).values[:, np.newaxis]\n",
    "            test = X_test.map(lambda x: len(re.findall(patt, x))>0).values[:, np.newaxis]\n",
    "            f_train.append(train)\n",
    "            f_test.append(test)\n",
    "    return np.concatenate((f_train), axis=1), np.concatenate((f_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_params = {\"lowercase\": True,\n",
    "             \"analyzer\": \"char_wb\",\n",
    "             \"stop_words\": None,\n",
    "             \"ngram_range\": (4, 4),\n",
    "             \"min_df\": 0.0,\n",
    "             \"max_df\": 1.0,\n",
    "             \"preprocessor\": None,#Preprocessor(),\n",
    "             \"max_features\": 4000,\n",
    "             \"norm\": \"l2\"*0,\n",
    "             \"use_idf\": 1\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Top N features\n",
    "# top = 100\n",
    "# r = tfidf_train.toarray().sum(axis=1)\n",
    "# topn_ids = np.argsort(r)[::-1][:top]\n",
    "# voc = [f for i,f in enumerate(features) if i not in topn_ids]\n",
    "# tf_params[\"vocabulary\"] = None#voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(**tf_params)\n",
    "tfidf_train = vectorizer.fit_transform(X_train)\n",
    "tfidf_test = vectorizer.transform(X_test)\n",
    "features = [\n",
    "            \"tfidf\", \n",
    "            \"length\",\n",
    "            \"phone\",\n",
    "            \"patt\",\n",
    "]\n",
    "train, test = build_features(X_train, X_test, features=features, vectorizer=vectorizer, var=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = vectorizer.get_feature_names()\n",
    "# dfs = top_feats_by_class(tfidf_train, y_train, features, min_tfidf=0.1, top_n=25)\n",
    "# plot_tfidf_classfeats_h(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it is much worse to misclassify ham\n",
    "SMS than letting spam pass the filter. So, it is desirable to be able to bias\n",
    "the filter towards classifying SMS as ham, yielding higher precision at the expense of recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(tf, X_test, clf, w=1.5):\n",
    "    probas = clf.predict_proba(X_test)\n",
    "    ratios = np.log(probas[:, 1] ) - np.log(probas[:, 0])\n",
    "    lengths = (tf.toarray()>0).sum(axis=1).T\n",
    "    thresholds = lengths * np.log(w)\n",
    "    y_pred = np.zeros_like(y_test)\n",
    "    y_pred[ratios>thresholds] = 1\n",
    "    return y_pred, ratios, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(min_samples_leaf=5, min_samples_split=15,\n",
    "                             n_estimators=100, max_depth=20, max_features=\"auto\", \n",
    "                             class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.02, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=25,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.997\n",
      "Recall: 0.967\n",
      "Precision: 0.962\n",
      "F1: 0.964\n",
      "Accuracy: 0.986\n",
      "\n",
      "Confusion matrix:\n",
      "      pred_ham  pred_spam\n",
      "ham       1453         14\n",
      "spam        12        352\n",
      "\n",
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      1467\n",
      "          1       0.96      0.97      0.96       364\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=25, class_weight=\"balanced\", \n",
    "                         C=0.02, penalty=\"l2\")\n",
    "#clf = MultinomialNB(alpha=0.01)#, class_prior=[0.5, 0.5])\n",
    "clf.fit(train, y_train)\n",
    "#pred, ratios, thresholds = predict_class(tfidf_test, test, clf, w=1.2)\n",
    "pred = clf.predict(test)\n",
    "proba = clf.predict_proba(test)[:, 1]\n",
    "output, report, conf_matrix = calc_metrics(y_test, pred, proba, labels=[\"ham\", \"spam\"], \n",
    "                                           print_=True, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3956    Ждем на выходных в Кувшине!Барашек на вертеле,...\n",
       "3469     \"OSCHADBANK\"  VASHU KARTKU ZABLOKOVANO.  Dlya...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "address                                                       NaN\n",
       "contact_name                                                  NaN\n",
       "label                                                           1\n",
       "resp                                                            1\n",
       "service                                                       NaN\n",
       "text             \"OSCHADBANK\"  VASHU KARTKU ZABLOKOVANO.  Dlya...\n",
       "timestamp                              2018-04-12 16:55:09.238000\n",
       "type                                                          NaN\n",
       "text_rep         \"OSCHADBANK\"  VASHU KARTKU ZABLOKOVANO.  Dlya...\n",
       "Name: 3469, dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[fn_i[:2]]\n",
    "total.loc[3469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEGO №ттн Нової Пошти 20400068786210 www.constructors.com.ua\n",
      "\n",
      "lifecell вітає в Україні! Дякуємо, що обрали нас для закордонної подорожі. Сподіваємось, Вам сподобались зручні та вигідні тарифи на роумінг від lifecell  та інноваційний додаток BiP разом з послугою \"Безкоштовний BiP у роумінгу\". Більше деталей: s.lifecell.ua/roam\n",
      "\n",
      "Вмем подарили автономные зарядки с логотипом айдиалс * crazy *\n",
      "\n",
      "Христос Воскрес!Нехай полишають негаразди у дні світлості цього свята!\n",
      "\n",
      "Prof-kosmetika.com.ua новая почта ттн 20450059818655\n",
      "\n",
      "LEGO №ттн Нової Пошти 20400065998296 www.constructors.com.ua\n",
      "\n",
      "Тарифи підвищено через збільшення попиту. Ви можете скасувати поїздку протягом 5 хвилин без плати за скасування. Будьте готові за п ять хвилин до прибуття автомобіля.\n",
      "\n",
      "Строк дії нарахованих МБ Інтернету для BiP за послугою \"Безкоштовний BiP у роумінгу\" закінчується 18.03.2018 включно (за київським часом). Далі будуть діяти базові тарифи роумінгу відповідно до країни перебування. Деталі: s.lifecell.ua/r\n",
      "\n",
      "Извините, замена авто! Ожидайте новое смс. Хорошего Вам дня!\n",
      "\n",
      "lifecell вітає в Україні! Дякуємо, що обрали нас для закордонної подорожі. Сподіваємось, Вам сподобались зручні та вигідні тарифи на роумінг від lifecell  та інноваційний додаток BiP разом з послугою \"Безкоштовний BiP у роумінгу\". Більше деталей: s.lifecell.ua/roam\n",
      "\n",
      "Ясно! Я вчера звонила один раз, и сегодня пару. Отдыхай,если получится!! До связи в Киеве!\n",
      "\n",
      "Kontrol kachestva Lamoda: esli u vas byla problema s zakazom, otpravte 1 v otvet, i my perezvonim vam, ili napishite nam na problema@lamoda.ua.\n",
      "\n",
      "Karim, скачайте наше приложение и получите подтверждение бронирования на телефон! booking.com/App-7vD7ECUN (ссылка скоро перестанет действовать)\n",
      "\n",
      "Shanovnyy kliyente! Cherez tekhnichnyy roboty u Banku, zmina limitu po kartrakhunku bude nemogluva 07.04.18. Prynosymo svoyi vybachennya.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp_i = np.where((pred==1) & (y_test==0))[0]\n",
    "fn_i = np.where((pred==0) & (y_test==1))[0]\n",
    "for el in X_test.iloc[fp_i].values:\n",
    "    print(el+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsquash(X):\n",
    "    ''' (n,) -> (n,1) '''\n",
    "    if len(X.shape) == 1 or X.shape[0] == 1:\n",
    "        return np.asarray(X).reshape((len(X), 1))\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(X):\n",
    "    ''' (n,1) -> (n,) '''\n",
    "    return np.squeeze(np.asarray(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(TransformerMixin):\n",
    "    '''Base class for pure transformers'''\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squash(Transformer):\n",
    "    def transform(self, X, **kwargs):\n",
    "        return squash(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unsquash(Transformer):\n",
    "    def transform(self, X, **kwargs):\n",
    "        return unsquash(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTransformer(TransformerMixin):\n",
    "    ''' Use model predictions as transformer '''\n",
    "    def __init__(self, model, probs=True):\n",
    "        self.model = model\n",
    "        self.probs = probs\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict(model=self.model, probs=self.probs)\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        if self.probs:\n",
    "            pred = self.model.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            pred = self.model.predict(X)\n",
    "        return unsquash(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter(Transformer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            return X\n",
    "        elif isinstance(X, pd.Series):\n",
    "            return X.values\n",
    "        elif isinstance(X, str):\n",
    "            return np.array([X])\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(Transformer):\n",
    "    def __init__(self, use_tfidf=True):\n",
    "        self.use_tfidf = use_tfidf\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"use_tfidf\": self.use_tfidf}\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        if self.use_tfidf:\n",
    "            res = (X>0).sum(axis=1)\n",
    "        else:\n",
    "            res = np.vectorize(len)(X)\n",
    "        return unsquash(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdfLen(Transformer):\n",
    "    def __init__(self, add_len=True, **tfidf_params):\n",
    "        self.add_len = add_len\n",
    "        self.tfidf_params = tfidf_params.copy()\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        output = self.tfidf_params\n",
    "        output.update({\"add_len\": self.add_len})\n",
    "        return output\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        self.tfidf_params.update(**params)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.add_len = self.tfidf_params.pop(\"add_len\", self.add_len)\n",
    "        self.vectorizer = TfidfVectorizer(**self.tfidf_params)\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        res = self.vectorizer.transform(X)\n",
    "        if self.add_len:\n",
    "            lens = (res > 0).sum(axis=1)\n",
    "            res = sparse.hstack([res, lens]).tocsr()\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x65 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 65 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = X_test.iloc[:1]#.values\n",
    "l = TfIdfLen(add_len=1, **tf_params)\n",
    "l.fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchPattern(Transformer):\n",
    "    \n",
    "    def __init__(self, pattern, is_len, flags=re.U):\n",
    "        self.pattern = pattern\n",
    "        self.is_len = is_len\n",
    "        self.flags = flags\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(pattern=self.pattern, is_len=self.is_len, flags=self.flags)\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        if self.is_len:\n",
    "            func = lambda text: len(re.findall(self.pattern, text, self.flags))\n",
    "        else:\n",
    "            func = lambda text: bool(re.search(self.pattern, text, self.flags))\n",
    "        rez = np.vectorize(func)(X).astype(int)\n",
    "        return unsquash(rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleBinaryClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "\n",
    "    def __init__(self, mode, weights=None):\n",
    "        self.mode = mode\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        ''' Predict (weighted) probabilities '''\n",
    "        probs = np.average(X, axis=1, weights=self.weights)\n",
    "        return np.column_stack((1-probs, probs))\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Predict class labels. '''\n",
    "        if self.mode == 'average':\n",
    "            return binarize(self.predict_proba(X)[:,[1]], 0.5)\n",
    "        else:\n",
    "            res = binarize(X, 0.5)\n",
    "            return np.apply_along_axis(lambda x: np.bincount(x.astype(int), self.weights).argmax(), axis=1, arr=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ensemble(model_list, estimator=None):\n",
    "    models = []\n",
    "    for i, model in enumerate(model_list):\n",
    "        models.append(('model_transform'+str(i), ModelTransformer(model)))\n",
    "\n",
    "    if not estimator:\n",
    "        return FeatureUnion(models)\n",
    "    else:\n",
    "        return Pipeline([\n",
    "            ('features', FeatureUnion(models)),\n",
    "            ('estimator', estimator)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_pipe(add_len=True, tfidf_params={}):\n",
    "    vectorizer = TfIdfLen(add_len, **tfidf_params)\n",
    "    vec_pipe = [\n",
    "        ('vec', vectorizer)]\n",
    "    return Pipeline(vec_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_pipe(patterns):\n",
    "    pipes = []\n",
    "    for i, (patt, params) in enumerate(patterns):\n",
    "        kwargs = params.copy()\n",
    "        name = kwargs.pop(\"name\") + \"_\" + str(i)\n",
    "        transformer = MatchPattern(pattern=patt, **kwargs)\n",
    "        pipes.append((name, transformer))\n",
    "    return pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_pipe(use_tfidf=True, vec_pipe=None):\n",
    "    len_pipe = [(\"length\", Length(use_tfidf))]\n",
    "    if use_tfidf:\n",
    "        len_pipe.insert(0, (\"vec\", vec_pipe))\n",
    "    return Pipeline(len_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERNS = [(r\"[\\(\\d][\\d\\s\\(\\)-]{8,15}\\d\", {\"name\": \"phone\",\n",
    "                                            \"is_len\": 0}),\n",
    "           (r\"%|taxi|скидк|цін\", {\"name\": \"custom\",\n",
    "                                  \"is_len\": 0,\n",
    "                                  \"flags\": re.I | re.U})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform_pipe(tf_params, add_len=True, vec_mode=\"add\"):\n",
    "    vec_pipe = get_vec_pipe(add_len, tf_params)\n",
    "    if vec_mode == \"only\":\n",
    "        return vec_pipe\n",
    "    patt_pipe = get_pattern_pipe(PATTERNS)\n",
    "    chain = [\n",
    "        ('converter', Converter()),\n",
    "        ('union', FeatureUnion([\n",
    "            ('vec', vec_pipe),\n",
    "            *patt_pipe\n",
    "        ]))\n",
    "    ]\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(name, seed=25):\n",
    "    if name == \"logit\":\n",
    "        model = LogisticRegression(C=1, class_weight=\"balanced\", random_state=seed, penalty=\"l2\")\n",
    "        model.grid_s = {f'{name}__C' : (0.1, 0.2, 0.3, 0.4, 0.5, 1, 5, 10)}\n",
    "        model.grid_b = {f'{name}__C' : [(1)]}\n",
    "    elif name == \"nb\":\n",
    "        model = MultinomialNB(alpha=0.1) #class_prior=[0.5, 0.5])\n",
    "        model.grid_s = {f'{name}__alpha' : (0.1, 0.5, 1, 5, 10)}\n",
    "        model.grid_b = {f'{name}__alpha' : [(1)]}\n",
    "    model.name = name\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator_pipe(name, model, tf_params, vec_mode=\"add\"):\n",
    "    chain = build_transform_pipe(tf_params, vec_mode=vec_mode)\n",
    "    chain.append((name, model))\n",
    "    pipe = Pipeline(chain)\n",
    "    pipe.name = name\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_classifiers(names):\n",
    "    return [build_classifier(name) for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = [\"logit\", \"nb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_pipes(tf_params, vec_mode=\"add\", names=NAMES):\n",
    "    clfs = get_all_classifiers(names)\n",
    "    return [get_estimator_pipe(clf.name, clf, tf_params, vec_mode) for clf in clfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = data.loc[data.text.notnull()]\n",
    "    data[\"text\"] = data[\"text\"].str.replace(r\"[\\n\\r]+\", \"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename=file_out):\n",
    "    data = pd.read_excel(data_dir / filename)\n",
    "    data = preprocess(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(tf_params, filename=file_out, random_state=25, vec_mode=\"all\",\n",
    "                n_splits=5, log=True, grid=\"grid_s\", transformer_grid={},\n",
    "                scoring=\"f1\", estimator_names=NAMES):\n",
    "    \n",
    "    data = load_data(filename)\n",
    "    X, y = data[\"text\"], data[\"label\"]\n",
    "    cv_splitter = StratifiedKFold(n_splits=n_splits, random_state=random_state, \n",
    "                                  shuffle=True)\n",
    "    # Build pipelines\n",
    "    pipes = build_all_pipes(tf_params, names=estimator_names, vec_mode=vec_mode)\n",
    "    \n",
    "    best = []\n",
    "    best_scores = []\n",
    "    for i, pipe in enumerate(pipes):\n",
    "        if log:\n",
    "            print(f\"Hypertuning model {i+1} out of {len(pipes)}: {pipe.name}\")\n",
    "            print(\"================================================================================\")\n",
    "\n",
    "        current_grid = getattr(pipe.steps[-1][1], grid)\n",
    "        current_grid.update(transformer_grid)\n",
    "        gs = GridSearchCV(pipe, current_grid, scoring=scoring, cv=cv_splitter, n_jobs=-1, verbose=False)\n",
    "        model = gs.fit(X, y)\n",
    "        \n",
    "        if log:\n",
    "            print(f\"Best score on training set (CV): {gs.best_score_:0.3f}\" )\n",
    "            print(\"Best parameters set:\")   \n",
    "            for params, mean_score, scores in gs.grid_scores_:\n",
    "                print(f\"{mean_score:0.4f} (+/-{scores.std() / 2:0.4f}) for {params}: {scores}\")\n",
    "        best.append(gs.best_estimator_)\n",
    "        temp = [el for el in gs.grid_scores_ if el.parameters==gs.best_params_][0]\n",
    "        best_scores.append({\"params\": temp[0], \"mean\": temp[1], \"scores\": temp[-1],\n",
    "                            \"std\": temp[-1].std()})\n",
    "    return best, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tf = {#\"union__vec__vec__use_idf\": [0, 1],\n",
    "           #\"union__vec__vec__ngram_range\": [(3,3), (4,4), (5,5), (3,5), (3,4)],\n",
    "           #\"union__vec__vec__max_features\": range(2000, 4500, 500)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypertuning model 1 out of 2: logit\n",
      "================================================================================\n",
      "Best score on training set (CV): 0.955\n",
      "Best parameters set:\n",
      "0.9552 (+/-0.0022) for {'logit__C': 0.1}: [0.95454545 0.95       0.9519833  0.96296296 0.95634096]\n",
      "0.9547 (+/-0.0035) for {'logit__C': 0.2}: [0.95670103 0.94560669 0.9519833  0.96694215 0.95218295]\n",
      "0.9547 (+/-0.0035) for {'logit__C': 0.3}: [0.95670103 0.94560669 0.9519833  0.96694215 0.95218295]\n",
      "0.9550 (+/-0.0036) for {'logit__C': 0.4}: [0.95867769 0.94537815 0.9519833  0.96694215 0.95218295]\n",
      "0.9550 (+/-0.0036) for {'logit__C': 0.5}: [0.95867769 0.94537815 0.9519833  0.96694215 0.95218295]\n",
      "0.9533 (+/-0.0042) for {'logit__C': 1}: [0.95652174 0.94291755 0.9539749  0.96694215 0.94605809]\n",
      "0.9503 (+/-0.0038) for {'logit__C': 5}: [0.95435685 0.93842887 0.9519833  0.96049896 0.94605809]\n",
      "0.9502 (+/-0.0039) for {'logit__C': 10}: [0.95435685 0.93842887 0.9539749  0.96033403 0.94409938]\n",
      "Hypertuning model 2 out of 2: nb\n",
      "================================================================================\n",
      "Best score on training set (CV): 0.852\n",
      "Best parameters set:\n",
      "0.8518 (+/-0.0102) for {'nb__alpha': 0.1}: [0.83450704 0.84135472 0.86131387 0.8342246  0.88764045]\n",
      "0.8497 (+/-0.0099) for {'nb__alpha': 0.5}: [0.83362522 0.83392226 0.86181818 0.83597884 0.88311688]\n",
      "0.8451 (+/-0.0104) for {'nb__alpha': 1}: [0.82926829 0.82578397 0.86025408 0.83157895 0.87867647]\n",
      "0.8276 (+/-0.0113) for {'nb__alpha': 5}: [0.81367521 0.81646655 0.83597884 0.80405405 0.86799277]\n",
      "0.8158 (+/-0.0123) for {'nb__alpha': 10}: [0.79932546 0.80541455 0.82495667 0.79008264 0.85918004]\n"
     ]
    }
   ],
   "source": [
    "best_estimators, best_scores = grid_search(transformer_grid=grid_tf, tf_params=tf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'logit__C': 0.1},\n",
       "  'mean': 0.9551650645693002,\n",
       "  'scores': array([0.95454545, 0.95      , 0.9519833 , 0.96296296, 0.95634096]),\n",
       "  'std': 0.004458147006864015},\n",
       " {'params': {'nb__alpha': 0.1},\n",
       "  'mean': 0.851805146456668,\n",
       "  'scores': array([0.83450704, 0.84135472, 0.86131387, 0.8342246 , 0.88764045]),\n",
       "  'std': 0.02045529803302403}]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sms = \"привіт заходь до нас у ввечері додому\"\n",
    "# ham, spam = pipe.predict_proba(sms)[0]\n",
    "# print(f\"Probability ham: {ham*100:0.3f}%\\nProbability spam: {spam*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_log = LogisticRegression(random_state=25, class_weight=\"balanced\",                          \n",
    "#                              C=0.1, penalty=\"l2\")\n",
    "# clf_nb = MultinomialNB(alpha=0.1)#, class_prior=[0.5, 0.5])\n",
    "# clf_ensem = EnsembleBinaryClassifier(mode=\"averag\", weights=[5, 5])  \n",
    "\n",
    "# chain = build_transform_pipe(tf_params)\n",
    "# chain.append(('estimator', clf_log))\n",
    "# pipe = Pipeline(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.fit(X_train, y_train)\n",
    "# pred = pipe.predict(X_test)\n",
    "# proba = pipe.predict_proba(X_test)[:, 1]\n",
    "# output, report, conf_matrix = calc_metrics(y_test, pred, proba, labels=[\"ham\", \"spam\"], \n",
    "#                                            print_=True, mode=\"binary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
